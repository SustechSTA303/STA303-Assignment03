{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb 单元格 1\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m rl_model \u001b[39m=\u001b[39m QLearning(num_states, num_actions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m train_rl_model(stations, rl_model)\n",
      "\u001b[1;32m/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb 单元格 1\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     next_state \u001b[39m=\u001b[39m state\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m rl_model\u001b[39m.\u001b[39mupdate_q_table(state, action, reward, next_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m start_station \u001b[39m=\u001b[39m next_station\n",
      "\u001b[1;32m/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb 单元格 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_q_table\u001b[39m(\u001b[39mself\u001b[39m, state, action, reward, next_state):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscretize_state(state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     next_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscretize_state(next_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     best_next_action_value \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_table[next_state, :])\n",
      "\u001b[1;32m/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb 单元格 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdiscretize_state\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# Discretize the continuous state into integer values\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     discretized_state \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mclip(val \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_states \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)) \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m discretized_state\n",
      "\u001b[1;32m/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb 单元格 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdiscretize_state\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# Discretize the continuous state into integer values\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     discretized_state \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mclip(val \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_states \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)) \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mugi/course/2023Fall/AI/STA303-Assignment03/reinforcement.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m discretized_state\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2180\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_clip_dispatcher)\n\u001b[1;32m   2112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclip\u001b[39m(a, a_min, a_max, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   2113\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2114\u001b[0m \u001b[39m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2115\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2178\u001b[0m \n\u001b[1;32m   2179\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2180\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m'\u001b[39m, a_min, a_max, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(asarray(obj), method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/_methods.py:161\u001b[0m, in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, casting, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m _clip_dep_invoke_with_casting(\n\u001b[1;32m    159\u001b[0m         um\u001b[39m.\u001b[39mmaximum, a, \u001b[39mmin\u001b[39m, out\u001b[39m=\u001b[39mout, casting\u001b[39m=\u001b[39mcasting, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[39mreturn\u001b[39;00m _clip_dep_invoke_with_casting(\n\u001b[1;32m    162\u001b[0m         um\u001b[39m.\u001b[39mclip, a, \u001b[39mmin\u001b[39m, \u001b[39mmax\u001b[39m, out\u001b[39m=\u001b[39mout, casting\u001b[39m=\u001b[39mcasting, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/_methods.py:115\u001b[0m, in \u001b[0;36m_clip_dep_invoke_with_casting\u001b[0;34m(ufunc, out, casting, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m# try to deal with broken casting rules\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m ufunc(\u001b[39m*\u001b[39margs, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    116\u001b[0m \u001b[39mexcept\u001b[39;00m _exceptions\u001b[39m.\u001b[39m_UFuncOutputCastingError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    117\u001b[0m     \u001b[39m# Numpy 1.17.0, 2019-02-24\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    119\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConverting the output of clip from \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPass `casting=\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39munsafe\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m` explicitly to silence this warning, or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from build_data import build_data\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=0.9, exploration_prob=0.2):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_prob = exploration_prob\n",
    "        self.q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        # Discretize the continuous state into integer values\n",
    "        discretized_state = tuple(int(np.clip(val * 100, 0, self.num_states - 1)) for val in state)\n",
    "        return discretized_state\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = self.discretize_state(state)\n",
    "        if np.random.rand() < self.exploration_prob:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state, :])\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        state = self.discretize_state(state)\n",
    "        next_state = self.discretize_state(next_state)\n",
    "\n",
    "        best_next_action_value = np.max(self.q_table[next_state, :])\n",
    "        td_error = reward + self.discount_factor * best_next_action_value - self.q_table[state, action]\n",
    "        self.q_table[state, action] += self.learning_rate * td_error\n",
    "\n",
    "\n",
    "def distance(station1, station2):\n",
    "    # Euclidean distance between two stations\n",
    "    return ((station1.position[0] - station2.position[0]) ** 2 +\n",
    "            (station1.position[1] - station2.position[1]) ** 2) ** 0.5\n",
    "\n",
    "def get_cost(state, action, rl_model):\n",
    "    # Calculate cost using the Q-value from the trained Q-learning model\n",
    "    return -rl_model.q_table[state, action]\n",
    "\n",
    "def train_rl_model(map, rl_model, num_episodes=1000):\n",
    "    for _ in range(num_episodes):\n",
    "        start_station = np.random.choice(list(map.values()))\n",
    "        end_station = np.random.choice(list(map.values()))\n",
    "        state = (start_station.position[0], start_station.position[1],\n",
    "                 end_station.position[0], end_station.position[1])\n",
    "\n",
    "        while start_station != end_station:\n",
    "            action = rl_model.choose_action(state)\n",
    "\n",
    "            if action == 0:  # move to a neighboring station\n",
    "                next_station = np.random.choice(list(start_station.links))\n",
    "                next_state = (next_station.position[0], next_station.position[1],\n",
    "                              end_station.position[0], end_station.position[1])\n",
    "                reward = -distance(start_station, next_station)\n",
    "            else:  # stay at the current station\n",
    "                next_state = state\n",
    "                reward = 0\n",
    "\n",
    "            rl_model.update_q_table(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            start_station = next_station\n",
    "\n",
    "# Usage example:\n",
    "stations, _ = build_data()\n",
    "\n",
    "# Assuming you have the number of states and actions based on your representation\n",
    "num_states = 4\n",
    "num_actions = 2\n",
    "\n",
    "# Initialize the Q-learning model\n",
    "rl_model = QLearning(num_states, num_actions)\n",
    "\n",
    "# Train the model\n",
    "train_rl_model(stations, rl_model)\n",
    "\n",
    "# Now you can use rl_model to estimate the cost in the A* algorithm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
